\section{Deferred Proofs}
\label{sec:deferred-proofs}

{\color{red}\textbf{Note:} this section is under construction --- some proofs lack detail.}

\subsection{Spectral Radius Estimator}

Let $\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_p \geq 0$ be the eigenvalues of $\Sigma$ and $\hat\lambda_1 \ge \hat\lambda_2 \ge \cdots \ge \hat\lambda_p$ the eigenvalues of $\widehat S_n$.

To do so, I leverage the fact that our matrix norm can be expressed in terms of the covariance eigenvalues and lean heavily on PCA theory going back to \textcite{anderson-1963-pca-asymptotics}.
For an overview, see \textcite{jolliffe-2002-pca} or \textcite{zagidullina-2021-random-matrix-theory}.
See \textcite{anderson-2003-multivariate} for more.

\begin{proposition}[Consistency of sample eigenvalues]
	\label{prop:sample-eigenvalue-consistency}
	The sample eigenvalues $\hat \lambda_i$ are consistent estimates of the population eigenvalues $\lambda_i$.
\end{proposition}

\begin{proof}
	I prove the stronger notion of almost-sure convergence. The (strong) law of large numbers gives almost-sure convergence of the covariance matrix $\widehat S_n \overset{a.s.}{\to} \Sigma$. The set of eigenvalues is a continuous function of a matrices entries \cite[Ch. 2, Th. 5.14, p. 118]{kato-1980-perturbation}, so the continuous mapping theorem yields that $\hat \lambda_i \overset{a.s.}{\to} \lambda_i$.
\end{proof}

\begin{proposition}[Asymptotic normality of sample eigenvalues]
	\label{prop:sample-eigenvalue-normality}
	Let $X_1,\dots,X_n \in \mathbb{R}^p$ be i.i.d.\ with mean $0$ and covariance $\Sigma$.
	If $\Sigma$ has eigenvalues $\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_p \geq 0$ with corresponding sample eigenvalues $\hat\lambda_1 \ge \hat\lambda_2 \ge \cdots \ge \hat\lambda_p$, then
	\begin{equation}
		\sqrt{n} \paren{\hat\lambda_i - \lambda_i}
		\overset{d}{\longrightarrow}
		N\!\paren{0,\;\sigma_i^2},
		\qquad i=1,\dots,p.
	\end{equation}
	If $\set{X_i}$ are normally distributed, then $\sigma_i^2 = 2\lambda_i^2$. If not,
	$\sigma_i^2$ depends (somewhat complexly) on the fourth cumulants of $X_i$.
\end{proposition}

\begin{proof}
	The normal result is due to \textcite{anderson-1963-pca-asymptotics}.
	The non-normal case is due to \textcite{waternaux-1976-nonnormal-eigenvalues} and expanded upon by \textcite{eaton-1991-eigenvalues}.
	The non-normal case will be of most use to us because most survey data is decidedly not normal (e.g. binary response or multiple choice).
\end{proof}

Asymptotic normality and consistency of the spectral radius estimator follow trivially from \cref{prop:sample-eigenvalue-consistency} and \cref{prop:sample-eigenvalue-normality}.

\subsection{Latent Model}

I first establish a few lemmas that greatly simplify the proof.
The first is regarding the spectral radius of a positive rank-one update to a positive semidefinite matrix, which I prove as consequence of Weyl's inequalities \parencite{weyl-1912-inequalities,tao-2010-eigenvalue-blog}.
This result is very similar in flavor those of \textcite{golub-1973-eigenvalues}.
I then leverage this to prove our proposition and close with a discussion of two sufficient conditions under which the phrase "non-decreasing" on the last line of \cref{thm:one-dim-implies-multiple-dim} becomes "strictly increasing".

\begin{lemma}
	\label{lem:rank-one-update}
	If $D \in \R^{n \times n}$ is symmetric and positive semidefinite, $v \in \R^n$, and $c \geq 0$, then
	\begin{equation}
		\rho(c v v \trans + D) \geq \rho(D).
	\end{equation}
\end{lemma}

\begin{proof}
	See \textcite[\S 5]{golub-1973-eigenvalues}.
	% \todo{do I want to spell out the proof from Weyl's inequalities?
	% https://nhigham.com/2021/03/09/eigenvalue-inequalities-for-hermitian-matrices/
\end{proof}

\begin{lemma}
	\label{lem:rank-one-update-increasing}
	Let the setup be the same as in \cref{lem:rank-one-update}. Then $\rho(c v v \trans + D)$ is non-decreasing with respect to $c$.
\end{lemma}

\begin{proof}
	Let $\epsilon > 0$. Then,
	\begin{equation}
		\begin{aligned}
			\rho((c + \epsilon) v v \trans + D)
			\ = \ \rho(\epsilon v v \trans + (c v v \trans + D))
			\ \geq \ \rho(c v v \trans + D)
		\end{aligned}
	\end{equation}
	by \cref{lem:rank-one-update} because $c v v \trans + D \succeq 0$ by the closure of positive semidefinite matrices under addition.
\end{proof}

Proof of the original result is now quite simple:

\begin{proof}[Proof of \cref{thm:one-dim-implies-multiple-dim}]
	The spectral radius of $\Sigma$ is
	\begin{equation}
		\rho(\Sigma)
		\ = \ \rho(\Var(\beta x + \epsilon))
		\ = \ \rho(a \beta \beta \trans + \Gamma)
	\end{equation}
	Because $\Gamma$ is a covariance matrix, it's symmetric and positive semidefinite and \cref{lem:rank-one-update-increasing} gives us that $\rho(\sigma)$ is non-decreasing in $a$ as desired.
\end{proof}

Strict increase if $\beta$ nontrivially projects onto the first eigenspace of $\Gamma$. This happens on all but a measure zero subset of possible $(\beta, \Gamma)$ pairs.

\begin{proposition}
	If $a = \Var(x) > \rho(\Gamma) = r$, then $r$ is \emph{strictly increasing} with respect to $a$.
\end{proposition}

\begin{corollary}
	There exists some $a > 0$ after which $r$ is strictly increasing.
\end{corollary}
