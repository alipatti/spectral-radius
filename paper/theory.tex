\section{Measuring Polarization}

I first intuitively describe

\subsection{On a Single Issue}

We begin by thinking about polarization on a single issue.
An initial important observation is that prevalence of extreme opinions alone is insufficient to measure what one intuitively thinks of as polarization.
For example, a question asking respondents to rate their support of slavery would get nearly universal strong opposition, yet this is not a particularly polarizing issue.
Polarization requires disagreement, i.e., spread in the distribution of responses.

The simplest measure of a distribution's spread (and the one we proceed with for the remainder of this paper) is its variance.
On distributions with finite and bounded support (which is most survey questions), this closely matches one's intuition about what constitutes a highly-polarized distribution.
For example, on binary a binary yes-or-no question, variance is maximized when responses are evenly split between the two options.
For a question asking one to numerically rank their agreement or disagreement with a particular statement, variance is maximized when responses are evenly split between the two extremes.
More generally, for any finite and discrete $A \subseteq \R$, the random variable with support over $A$ that has the highest variance has mass
equal to a half at the minimum and maximum values of $A$ and zero elsewhere.
% \begin{equation}
% 	f(x) = \begin{cases}
% 		\sfrac12 & \text{if $x = \max X$} \\
% 		\sfrac12 & \text{if $x = \min X$} \\
% 		0        & \otherwise.
% 	\end{cases}
% \end{equation}
One downside to using variance as the basis of our analysis is that the exact numeric values are quite meaningless: One can get any variance they like simply by rescaling the responses.
However, for a fixed scale, we can still get insight into the evolution of polarization over time.

\inlinetodo{get some citations from the poli sci literature to back up this inutution that spread => polarization}

\inlinetodo{Connect tail heaviness to variance for discrete bounded distributions}

\subsection{Across Multiple Issues}

In reality people hold opinions on a wide variety of issues, even within a single topic.
For example, regarding abortion, people hold separate opinions on whether abortion should be legal unilaterally, in the case of rape, in the case of incest, past the first trimester, or if the mother's health is at risk.
A good measure of polarization on abortion issues should take into account the spread of public opinion on all these scenarios.

A complete but high dimensional measure would be the distribution's covariance matrix $\Sigma$, however, this quickly becomes difficult to interpret as the number of issues grows.
We'd like some real, scalar summary measure.
A first attempt at reducing the dimensionality of our polarization index is to simply take the sum of the variances on each individual issue (i.e., the covariance matrix's trace).
This is a good start, but it fails to take into account any of the correlational structure between responses.
Indeed, much of the political science literature notes that a key feature of a ideologically-polarized society is a high-degree of correlation between opinions on separate issues, i.e., a society that is  ``locked in a zero-sum struggle along a single `us-vs-them' dimension'' \parencite{drutman-2020-doom-loop}.

A counterintuitive, but ultimately fruitful, idea is to abstract away entirely from the specifics of our polarization setting and instead think about the goal in an abstract mathematical sense: We have some object (our covariance matrix) in a high-dimensional vector space, and we'd like some scalar measure of its ``size''.
The mathematical concept of a norm provides exactly this.
In $\R^n$, one typically uses the Euclidian ($\mathcal L_2$) norm, but unfortunately in matrix spaces, there are many choices choices for a norm with \emph{a priori} no obvious ``best'' option.
We've already met one---the trace of a matrix is indeed a norm called the \emph{trace}, \emph{Ky Fan}, or \emph{nuclear norm}.
We've already seen the trace norm's shortcomings, so we focus on two other options: the spectral norm and the Frobenius norm.

There are many equivalent ways to define the spectral norm $\norm \Sigma_2$\footnote{
	For example: as the largest signular value; as the $\mathcal L_\infty$ norm of the singular values; as the $\mathcal L_2$ operator norm.
}---the most conducive to our setting is as the largest eigenvalue of the covariance matrix.\footnote{
	This is equivalent to the spectral norm because covariance matrices are always positive semidefinite.
}
The benefit of this definition is that the spectral norm is now easily interpretable as the variance of the responses' first principal component \parencite{anderson-1963-pca-asymptotics, jolliffe-2002-pca} and therefore combines information about the dispersion on individual issues with how much the dispersion projects onto a single dimension.
We explore this interpretation further in \cref{sec:trace-concentration}.

The Frobenius norm $\norm{\Sigma}_F$ also can be understood in terms of principal components:
If $\lambda_1, \ldots, \lambda_p$ are the eigenvalues of $\Sigma$ (equivalently, the variances of each of the principal components), the Frobenius norm can be written as $\sqrt{\lambda_1^2 + \cdots + \lambda_p^2}$, the Euclidian norm of the eigenvalue vector.
In some sense, this is a ``smoother'' summary of $\Sigma$'s eigenvalues than the spectral norm which simply picks out the largest.
% Instead, the Frobenius norm performs a smooth 
\inlinetodo{say more here?}

\Cref{fig:gss-example} provides an illustration of each of these norms applied to survey responses on two issues from the General Social Survey \parencite{gss}.
\cref{fig:norm-comparison} summarizes how each norm relates to the eigenvalues of the covariance matrix.
We elect to use the spectral norm for the remainder of the paper due to its simple interpretation as the variance of the first principal component, but our qualitative findings hold almost identically for other norm choices.

\begin{figure}
	\centering
	\caption{Comparison of Matrix Norms}
	\label{fig:norm-comparison}
	\begin{minipage}{.5\textwidth}
		\includegraphics[width=\textwidth]
		{./figures/examples/motivation/norm_illustration.pdf}%
	\end{minipage}%
	\hspace{.5in}
	\begin{tabular}{r l | c | c}
		\multicolumn{2}{c|}{Norm} & Expression       & $p$                           \\
		\hline
		Spectral                  & $\norm \Sigma_2$ & $\max(a, b)$       & $\infty$ \\
		Nuclear                   & $\norm \Sigma_*$ & $a + b$            & $1$      \\
		Frobenius                 & $\norm \Sigma_F$ & $\sqrt{a^2 + b^2}$ & $2$      \\
	\end{tabular}
	\notes{
		The left-hand panel shows the ellipse $\set{ \mathbf x \trans \Sigma \mathbf x : \norm{\mathbf x} = 1 }$ induced by the matrix $\Sigma = \operatorname{diag}(2, 1)$.
		The right-hand panel shows how our three matrix norms can be expressed as $p$-norms of the the matrix's eigenvalue vector, $(a, \, b) \trans$.
		The spectral norm measures the maximum radius of the data elipsoid, the nulear norm measures the sum of all radii, and the Frobenius norm provides a smooth in-between.
	}
\end{figure}

\begin{figure}
	\centering
	\caption{Responses to Two Related Survey Questions}
	\label{fig:gss-example}
	\includegraphics{./figures/examples/motivation/example.pdf}
	\notes{
		This figure plots 100 yearly responses from the General Social Survey on two questions regarding the responsibility of government to help those who are sick ($x$-axis) or who are poor ($y$-axis).
		Superimposed is the yearly covariance ellipse $\set{ \mathbf x \trans \Sigma \mathbf x : \norm{\mathbf x} = 1 }$.
		The position of each point has been slightly jittered for plotting.
		%
		From 2000 to 2012, the distribution of responses stretches causing an increase in all three of our measures of polarization.
		From 2012 to 2024, the entire distribution shifts upwards and to the right, causing an increase in the number of extreme responses.
		But the spread remains constant and our polarization measures do not budge.
	}
\end{figure}

\subsection{Statistical Formalization and Notation}
\label{sec:formalization}

We now we formalize our data setup and estimand, present an estimator, and prove some basic properties about it.
%
In particular, we assume that we observe i.i.d. samples $\mathbf x_1, \ldots, \mathbf x_n \sim \mathcal F$ with $\mathbf x_i = \paren{x_{i1}, \ldots, x_{ip}}$, drawn from some underlying distribution of political opinions $\mathcal F$.
Notably, we make no assumptions about the distribution $\mathcal F$ from which the responses ar drawn except that it has a finite covariance matrix $\Sigma = \Var(x_i)$.
Let $\lambda_1, \ldots, \lambda_p$ be the eigenvalues of $\Sigma$ ordered from largest to smallest.
Then, the estimand of interest is $\rho = \norm{\Sigma}_2 = \lambda_1$, the spectral radius of $\Sigma$.

My estimator is quite straightforward:
First, I define the sample covariance matrix $\widehat S_n \ceq \frac 1n \sum_i \mathbf x_i \mathbf x_i \trans$ and corresponding sample eigenvalues $\hat \lambda_1, \ldots, \hat \lambda_p$.
Then, my estimator is simply $\hat \rho = \| \widehat S_n \|_2 = \hat \lambda_1$.
This turns out to be okay (see \cref{sec:deferred-proofs} for proof):

\begin{proposition}
	The spectral radius of the sample covariance matrix is a consistent and asymptotically-normal estimator of the population spectral radius.
\end{proposition}

\subsection{Relationship to Spread in a Latent Ideology Model}

My setup is intentionally model-free, however, it does play nicely with a one-dimensional latent model (for example, a right/left political ideology).
In particular, the spectral radius picks up increases in spread within the latent distribution.
The following proposition formalizes that notion:

\begin{proposition}
	\label{thm:one-dim-implies-multiple-dim}
	Let $y \sim \mathcal G(a)$ be a person's latent one-dimensional idological position modeled as random variable with finite variance $a \in \R$.
	Let $\beta  = \paren{\beta_1, \ldots, \beta_p} \in \R^p$ be the sensitivity of each of issue to this latent ideology such that one's revealed policy positions are
	\begin{equation}
		x_{ij} = \beta_j y + e_i;
		\hspace{1in}
		\Var(\mathbf e) = \Gamma \in \R^{p \times p}.
	\end{equation}
	Define $\Sigma = \Var(\mathbf x_i)$ and $r = \norm{\Sigma}_2$.
	Then the following hold:
	\begin{enumerate}
		\item $r$ is non-decreasing in $a$;
		\item if $\beta$ nontrivially projects onto the principal eigenspace of $\Gamma$, then $r$ is strictly increasing in $a$;
		\item if $a > \norm{\Gamma}_2$, then $r$ is strictly increasing in $a$.
	\end{enumerate}
\end{proposition}

Note that this proposition is quite flexible: we make no assumptions about the latent distribution other than it has a finite second moment.
Additionally, the error term is also allowed to take any form---in particular it can have complex non-diagonal covariance structure.
See \cref{sec:deferred-proofs} for proof.
